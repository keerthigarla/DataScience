{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Case Study: Sentiment Analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data Prep"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Read in the data\n",
      "#df = pd.read_csv('train_5500.txt', sep=None)\n",
      "df = pd.read_csv('train_5500.txt', sep='delimiter',header=None, names = 'q')\n",
      "\n",
      "\n",
      "df1 = df.join(df['q'].str.split(':', expand=True).rename(columns={0:'classes', 1:'question'}))\n",
      "df2 = df1.drop(['q', 2, 3], axis=1)\n",
      "df2.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>classes</th>\n",
        "      <th>question</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>DESC</td>\n",
        "      <td>manner How did serfdom develop in and then lea...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>ENTY</td>\n",
        "      <td>cremat What films featured the character Popey...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>DESC</td>\n",
        "      <td>manner How can I find a list of celebrities ' ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>ENTY</td>\n",
        "      <td>animal What fowl grabs the spotlight after the...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>ABBR</td>\n",
        "      <td>exp What is the full form of .com ?</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "  classes                                           question\n",
        "0    DESC  manner How did serfdom develop in and then lea...\n",
        "1    ENTY  cremat What films featured the character Popey...\n",
        "2    DESC  manner How can I find a list of celebrities ' ...\n",
        "3    ENTY  animal What fowl grabs the spotlight after the...\n",
        "4    ABBR                exp What is the full form of .com ?"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df2.isnull().sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "classes     0\n",
        "question    0\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#calculate the number of labels for each class\n",
      "df2.classes.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "ENTY    1250\n",
        "HUM     1223\n",
        "DESC    1162\n",
        "NUM      896\n",
        "LOC      835\n",
        "ABBR      86\n",
        "Name: classes, dtype: int64"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df2.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "(5452, 2)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Split data into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(df2['question'],df2['classes'],random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#calculate the number of labels for each class\n",
      "y_train.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "ENTY    935\n",
        "HUM     927\n",
        "DESC    878\n",
        "NUM     672\n",
        "LOC     621\n",
        "ABBR     56\n",
        "Name: classes, dtype: int64"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('X_train first entry:\\n\\n', X_train.iloc[0])\n",
      "print('\\n\\nX_train shape: ', X_train.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "X_train first entry:\n",
        "\n",
        " count How much in miles is a ten K run ?\n",
        "\n",
        "\n",
        "X_train shape:  (4089,)\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#calculate average number of words per sentence\n",
      "import numpy as np\n",
      "np.mean([len(s.split(\" \")) for s in X_train])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "11.106138420151627"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Stemming"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#STEMMING we have 1. porter, 2. snowball, 3. wordnetlemmatizer. Lets use wordNetLemmatizer which is not exactly as stemming, \n",
      "#the lemmatizer needs the POS(part of speech) of the word, \u2018v\u2019 (for verb), to work, and even though it is much slower than the others, \n",
      "#it returns an actual english word. \n",
      "import nltk.stem\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "stemmer = PorterStemmer()\n",
      "def stem_tokens(tokens, stemmer):\n",
      "    stemmed = []\n",
      "    for item in tokens:\n",
      "        stemmed.append(stemmer.stem(item))\n",
      "        return stemmed\n",
      "#Tokenize\n",
      "import re\n",
      "def tokenize(text):\n",
      "    for row in text:\n",
      "        rtext =re.sub(\"[^a-zA-Z]\",\" \", row)\n",
      "        tokens = nltk.word_tokenize(rtext)\n",
      "        #stem\n",
      "        stems = stem_tokens(tokens, stemmer)\n",
      "        print(stems)\n",
      "\n",
      "#tokenize(X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "CountVectorizer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#stop words, remove them by adding to CountVectorizer\n",
      "from nltk.corpus import stopwords\n",
      "sw = stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print(tokens)\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "args = {\"stem\": False, \"lemmatize\": True}\n",
      "# Fit the CountVectorizer to the training data\n",
      "vect = CountVectorizer(analyzer = 'word', stop_words='english', lowercase=True).fit(X_train)\n",
      "#lambda text: tokenizer(text, **args)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyze = vect.build_analyzer()\n",
      "analyze(\"This is a text document to analyze.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "['text', 'document', 'analyze']"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vect\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
        "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "        tokenizer=None, vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = vect.get_feature_names()\n",
      "#[::2000]\n",
      "#print(vocab)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": false
     },
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(vect.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "6853"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# transform the documents in the training data to a document-term matrix\n",
      "X_train_vectorized_c = vect.transform(X_train)\n",
      "df_vect = pd.DataFrame(X_train_vectorized_c.toarray(), columns=vect.get_feature_names())\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_vect.head()\n",
      "df_vect.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "(4089, 6853)"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "# Train multinomial logistic regression\n",
      "classifier = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
      "model = classifier.fit(X_train_vectorized_c, y_train)\n",
      "\n",
      "\n",
      "# Train the model\n",
      "#model = LogisticRegression()\n",
      "#model.fit(X_train_vectorized, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 434
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "# Predict the transformed test documents\n",
      "predictions_c = model.predict(vect.transform(X_test))\n",
      "\n",
      "\n",
      "#print('AUC: ', roc_auc_score(y_test, predictions)) \n",
      "# AUC dest work for multiclass classification\n",
      "from sklearn import metrics\n",
      "metrics.precision_score(y_test, predictions_c, average='micro')  \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 435,
       "text": [
        "0.96184886280264126"
       ]
      }
     ],
     "prompt_number": 435
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.recall_score(y_test, predictions_c, average='micro')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 436,
       "text": [
        "0.96184886280264126"
       ]
      }
     ],
     "prompt_number": 436
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.f1_score(y_test, predictions_c, average='weighted')  \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 437,
       "text": [
        "0.9620095031708511"
       ]
      }
     ],
     "prompt_number": 437
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.fbeta_score(y_test, predictions_c, average='macro', beta=0.5)  \n",
      "metrics.precision_recall_fscore_support(y_test, predictions_c, beta=0.5, average=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 438,
       "text": [
        "(array([ 1.        ,  0.96271186,  0.95098039,  0.9965035 ,  0.88938053,  1.        ]),\n",
        " array([ 1.        ,  1.        ,  0.92380952,  0.96283784,  0.93925234,\n",
        "         0.98214286]),\n",
        " array([ 1.        ,  0.96994536,  0.9454191 ,  0.98958333,  0.89892665,\n",
        "         0.99637681]),\n",
        " array([ 30, 284, 315, 296, 214, 224]))"
       ]
      }
     ],
     "prompt_number": 438
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get the feature names as numpy array\n",
      "feature_names = np.array(vect.get_feature_names())\n",
      "\n",
      "# Sort the coefficients from the model\n",
      "sorted_coef_index = model.coef_[0].argsort()\n",
      "\n",
      "# Find the 10 smallest and 10 largest coefficients\n",
      "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
      "# so the list returned is in order of largest to smallest\n",
      "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
      "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Smallest Coefs:\n",
        "['desc' 'did' 'ind' 'def' 'count' 'manner' 'reason' 'world' 'gr' 'date']\n",
        "\n",
        "Largest Coefs: \n",
        "['exp' 'abb' 'stand' 'abbreviation' 'acronym' 'does' 'bureau'\n",
        " 'investigation' 'national' 'mean']\n"
       ]
      }
     ],
     "prompt_number": 439
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Tfidf - term frequency - inverse document frequency."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
      "vect_tfidf = TfidfVectorizer(min_df=5, stop_words='english',  lowercase = True).fit(X_train)\n",
      "len(vect_tfidf.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 440,
       "text": [
        "726"
       ]
      }
     ],
     "prompt_number": 440
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec = vect_tfidf.transform(X_train).toarray()\n",
      "\n",
      "print(vec)\n",
      "\n",
      "X_train_vectorized_t = vect_tfidf.transform(X_train)\n",
      "#print(X_train_vectorized)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " ..., \n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " [ 0.          0.64740893  0.         ...,  0.          0.          0.        ]\n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n"
       ]
      }
     ],
     "prompt_number": 441
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "model = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
      "model.fit(X_train_vectorized_t, y_train)\n",
      "\n",
      "predictions_t = model.predict(vect_tfidf.transform(X_test))\n",
      "\n",
      "#print('AUC: ', roc_auc_score(y_test, predictions)) AUROC doesnt not work for multiclass classification\n",
      "from sklearn import metrics\n",
      "metrics.precision_score(y_test, predictions_t, average='micro')  \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 442,
       "text": [
        "0.94057226705796038"
       ]
      }
     ],
     "prompt_number": 442
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.recall_score(y_test, predictions_t, average='micro')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 443,
       "text": [
        "0.94057226705796038"
       ]
      }
     ],
     "prompt_number": 443
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.f1_score(y_test, predictions_t, average='weighted')  \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 444,
       "text": [
        "0.94010180911784746"
       ]
      }
     ],
     "prompt_number": 444
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.fbeta_score(y_test, predictions_t, average='macro', beta=0.5)  \n",
      "metrics.precision_recall_fscore_support(y_test, predictions_t, beta=0.5, average=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 445,
       "text": [
        "(array([ 1.        ,  0.96219931,  0.83651226,  0.99300699,  0.95348837,  1.        ]),\n",
        " array([ 0.96666667,  0.98591549,  0.97460317,  0.95945946,  0.76635514,\n",
        "         0.97321429]),\n",
        " array([ 0.99315068,  0.96685083,  0.86090858,  0.98611111,  0.90909091,\n",
        "         0.99452555]),\n",
        " array([ 30, 284, 315, 296, 214, 224]))"
       ]
      }
     ],
     "prompt_number": 445
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 446,
       "text": [
        "(1363,)"
       ]
      }
     ],
     "prompt_number": 446
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_names = np.array(vect.get_feature_names())\n",
      "\n",
      "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
      "\n",
      "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
      "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Smallest tfidf:\n",
        "['000' 'athletes' 'athletic' 'atlantic' 'atlas' 'atm' 'atmosphere' 'atom'\n",
        " 'attack' 'attacks']\n",
        "\n",
        "Largest tfidf: \n",
        "['2000' 'alphabetically' 'boats' 'bendix' '576' 'associaton' 'bios'\n",
        " 'appeared' 'appearances' '1900s']\n"
       ]
      }
     ],
     "prompt_number": 447
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted_coef_index = model.coef_[0].argsort()\n",
      "\n",
      "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
      "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Smallest Coefs:\n",
        "['69' '55' 'agent' '50s' '32' 'andorra' 'beats' '47' 'associates' 'ado']\n",
        "\n",
        "Largest Coefs: \n",
        "['abdominal' '13' 'award' '137' '80' 'anybody' 'anesthetic' 'alpert'\n",
        " 'animal' '19th']\n"
       ]
      }
     ],
     "prompt_number": 448
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# These reviews are treated the same by our current model\n",
      "print(model.predict(vect_tfidf.transform(['not an issue, phone is working',\n",
      "                                    'an issue, phone is not working'])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['ENTY' 'ENTY']\n"
       ]
      }
     ],
     "prompt_number": 449
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "n-grams"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit the CountVectorizer to the training data specifiying a minimum \n",
      "# document frequency of 5 and extracting 1-grams and 2-grams\n",
      "vect = CountVectorizer(min_df=5, stop_words='english', lowercase=True, ngram_range=(1,2)).fit(X_train)\n",
      "\n",
      "X_train_vectorized_n = vect.transform(X_train)\n",
      "\n",
      "len(vect.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 452,
       "text": [
        "873"
       ]
      }
     ],
     "prompt_number": 452
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
      "model.fit(X_train_vectorized_n, y_train)\n",
      "\n",
      "predictions = model.predict(vect.transform(X_test))\n",
      "\n",
      "#print('AUC: ', roc_auc_score(y_test, predictions)) does not work for multiclass classification\n",
      "from sklearn import metrics\n",
      "metrics.precision_score(y_test, predictions, average='micro')  \n",
      "\n",
      "#metrics.recall_score(y_test, predictions, average='micro')\n",
      "#metrics.f1_score(y_test, predictions, average='weighted')  \n",
      "#metrics.fbeta_score(y_test, predictions, average='macro', beta=0.5)  \n",
      "#metrics.precision_recall_fscore_support(y_test, predictions, beta=0.5, average=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 453,
       "text": [
        "0.95524578136463678"
       ]
      }
     ],
     "prompt_number": 453
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.recall_score(y_test, predictions, average='micro')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 454,
       "text": [
        "0.95524578136463678"
       ]
      }
     ],
     "prompt_number": 454
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.f1_score(y_test, predictions, average='weighted')  \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 455,
       "text": [
        "0.95558090644265903"
       ]
      }
     ],
     "prompt_number": 455
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.fbeta_score(y_test, predictions, average='macro', beta=0.5)  \n",
      "metrics.precision_recall_fscore_support(y_test, predictions, beta=0.5, average=None)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 456,
       "text": [
        "(array([ 1.        ,  0.95945946,  0.94444444,  0.99649123,  0.86580087,  1.        ]),\n",
        " array([ 1.        ,  1.        ,  0.91746032,  0.95945946,  0.93457944,\n",
        "         0.95982143]),\n",
        " array([ 1.        ,  0.96730245,  0.93892138,  0.98885794,  0.87873462,\n",
        "         0.99169742]),\n",
        " array([ 30, 284, 315, 296, 214, 224]))"
       ]
      }
     ],
     "prompt_number": 456
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_names = np.array(vect.get_feature_names())\n",
      "\n",
      "sorted_coef_index = model.coef_[0].argsort()\n",
      "\n",
      "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
      "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Smallest Coefs:\n",
        "['desc' 'did' 'ind' 'def' 'count' 'manner' 'world' 'reason' 'gr' 'date']\n",
        "\n",
        "Largest Coefs: \n",
        "['exp' 'abb' 'abbreviation' 'exp does' 'national' 'stand' 'middle' 'air'\n",
        " 'business' 'way']\n"
       ]
      }
     ],
     "prompt_number": 457
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# These reviews are now correctly identified\n",
      "print(model.predict(vect.transform(['not an issue, phone is working',\n",
      "                                    'an issue, phone is not working'])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['LOC' 'LOC']\n"
       ]
      }
     ],
     "prompt_number": 458
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Support Vector Machine\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm, datasets\n",
      "\n",
      "svl = svm.SVC(kernel='linear', C=1, gamma=.1,  probability = False).fit(X_train_vectorized_c, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 459
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# we create an instance of SVM and fit out data. We do not scale our\n",
      "# data since we want to plot the support vectors\n",
      "svr = svm.SVC(kernel='rbf', C=1, gamma=.1,  probability = False).fit(X_train_vectorized_c, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 460
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "svr.score(X_train_vectorized_c, y_train)\n",
      "#Predict Output\n",
      "predicted= svr.predict(X_test)\n",
      "print(predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "could not convert string to float: 'other What is the frequency of VHF ?'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-463-b31870161b21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msvr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vectorized_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Predict Output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredicted\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msvr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \"\"\"\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'support_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'other What is the frequency of VHF ?'"
       ]
      }
     ],
     "prompt_number": 463
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Dimensionality Reduction with LDA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "\n",
      "data = pd.read_csv('reviews_sentiments.csv')\n",
      "\n",
      "\n",
      "\n",
      "cv = CountVectorizer(stop_words = 'english', max_features=250)\n",
      "cv_ft = cv.fit_transform(data.review_text)\n",
      "lda = LinearDiscriminantAnalysis()\n",
      "lda.fit(cv_ft.toarray(), data.sentiment)\n",
      "coef_df = pd.DataFrame({'word':cv.get_feature_names(),'coef':lda.coef_[0]}).sort_values(by='coef',ascending=False).reset_index(drop=True)\n",
      "#print(coef_df.head())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div>\n",
        "<style>\n",
        "    .dataframe thead tr:only-child th {\n",
        "        text-align: right;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: left;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>review_text</th>\n",
        "      <th>sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>Not sure who was more lost - the flat characte...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>Very little music or anything to speak of.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>The best scene in the movie was when Gerardo i...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>The rest of the movie lacks art, charm, meanin...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>Wasted two hours.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>Saw the movie today and thought it was a good ...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>A bit predictable.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td>Loved the casting of Jimmy Buffet as the scien...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>And those baby owls were adorable.</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>The movie showed a lot of Florida at it's best...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>The Songs Were The Best And The Muppets Were S...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>It Was So Cool.</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>This is a very \"right on case\" movie that deli...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td>It had some average acting from the main perso...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td>This review is long overdue, since I consider ...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td>I'll put this gem up against any movie in term...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td>It's practically perfect in all of them  a tru...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td>\" The structure of this film is easily the mos...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td>I can think of no other film where something v...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td>In other words, the content level of this film...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td>How can anyone in their right mind ask for any...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td>It's quite simply the highest, most superlativ...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td>Yes, this film does require a rather significa...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td>This short film certainly pulls no punches.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td>Graphics is far from the best part of the game.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27</th>\n",
        "      <td>This is the number one best TH game in the ser...</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28</th>\n",
        "      <td>It deserves strong love.</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td>It is an insane game.</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2970</th>\n",
        "      <td>I immediately said I wanted to talk to the man...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2971</th>\n",
        "      <td>The ambiance isn't much better.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2972</th>\n",
        "      <td>Unfortunately, it only set us up for disapppoi...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2973</th>\n",
        "      <td>The food wasn't good.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2974</th>\n",
        "      <td>Your servers suck, wait, correction, our serve...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2975</th>\n",
        "      <td>What happened next was pretty....off putting.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2976</th>\n",
        "      <td>too bad cause I know it's family owned, I real...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2977</th>\n",
        "      <td>Overpriced for what you are getting.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2978</th>\n",
        "      <td>I vomited in the bathroom mid lunch.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2979</th>\n",
        "      <td>I kept looking at the time and it had soon bec...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2980</th>\n",
        "      <td>I have been to very few places to eat that und...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2981</th>\n",
        "      <td>We started with the tuna sashimi which was bro...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2982</th>\n",
        "      <td>Food was below average.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2983</th>\n",
        "      <td>It sure does beat the nachos at the movies but...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2984</th>\n",
        "      <td>All in all, Ha Long Bay was a bit of a flop.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2985</th>\n",
        "      <td>The problem I have is that they charge $11.99 ...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2986</th>\n",
        "      <td>Shrimp- When I unwrapped it (I live only 1/2 a...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2987</th>\n",
        "      <td>It lacked flavor, seemed undercooked, and dry.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2988</th>\n",
        "      <td>It really is impressive that the place hasn't ...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2989</th>\n",
        "      <td>I would avoid this place if you are staying in...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2990</th>\n",
        "      <td>The refried beans that came with my meal were ...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2991</th>\n",
        "      <td>Spend your money and time some place else.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2992</th>\n",
        "      <td>A lady at the table next to us found a live gr...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2993</th>\n",
        "      <td>the presentation of the food was awful.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2994</th>\n",
        "      <td>I can't tell you how disappointed I was.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2995</th>\n",
        "      <td>I think food should have flavor and texture an...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2996</th>\n",
        "      <td>Appetite instantly gone.</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2997</th>\n",
        "      <td>Overall I was not impressed and would not go b...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2998</th>\n",
        "      <td>The whole experience was underwhelming, and I ...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2999</th>\n",
        "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
        "      <td>0.0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>3000 rows \u00d7 2 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "                                            review_text  sentiment\n",
        "0     A very, very, very slow-moving, aimless movie ...        0.0\n",
        "1     Not sure who was more lost - the flat characte...        0.0\n",
        "2     Attempting artiness with black & white and cle...        0.0\n",
        "3          Very little music or anything to speak of.          0.0\n",
        "4     The best scene in the movie was when Gerardo i...        1.0\n",
        "5     The rest of the movie lacks art, charm, meanin...        0.0\n",
        "6                                   Wasted two hours.          0.0\n",
        "7     Saw the movie today and thought it was a good ...        1.0\n",
        "8                                  A bit predictable.          0.0\n",
        "9     Loved the casting of Jimmy Buffet as the scien...        1.0\n",
        "10                 And those baby owls were adorable.          1.0\n",
        "11    The movie showed a lot of Florida at it's best...        1.0\n",
        "12    The Songs Were The Best And The Muppets Were S...        1.0\n",
        "13                                    It Was So Cool.          1.0\n",
        "14    This is a very \"right on case\" movie that deli...        1.0\n",
        "15    It had some average acting from the main perso...        0.0\n",
        "16    This review is long overdue, since I consider ...        1.0\n",
        "17    I'll put this gem up against any movie in term...        1.0\n",
        "18    It's practically perfect in all of them  a tru...        1.0\n",
        "19    \" The structure of this film is easily the mos...        1.0\n",
        "20    I can think of no other film where something v...        1.0\n",
        "21    In other words, the content level of this film...        1.0\n",
        "22    How can anyone in their right mind ask for any...        1.0\n",
        "23    It's quite simply the highest, most superlativ...        1.0\n",
        "24    Yes, this film does require a rather significa...        1.0\n",
        "25        This short film certainly pulls no punches.          0.0\n",
        "26    Graphics is far from the best part of the game.          0.0\n",
        "27    This is the number one best TH game in the ser...        1.0\n",
        "28                           It deserves strong love.          1.0\n",
        "29                              It is an insane game.          1.0\n",
        "...                                                 ...        ...\n",
        "2970  I immediately said I wanted to talk to the man...        0.0\n",
        "2971                    The ambiance isn't much better.        0.0\n",
        "2972  Unfortunately, it only set us up for disapppoi...        0.0\n",
        "2973                              The food wasn't good.        0.0\n",
        "2974  Your servers suck, wait, correction, our serve...        0.0\n",
        "2975      What happened next was pretty....off putting.        0.0\n",
        "2976  too bad cause I know it's family owned, I real...        0.0\n",
        "2977               Overpriced for what you are getting.        0.0\n",
        "2978               I vomited in the bathroom mid lunch.        0.0\n",
        "2979  I kept looking at the time and it had soon bec...        0.0\n",
        "2980  I have been to very few places to eat that und...        0.0\n",
        "2981  We started with the tuna sashimi which was bro...        0.0\n",
        "2982                            Food was below average.        0.0\n",
        "2983  It sure does beat the nachos at the movies but...        0.0\n",
        "2984       All in all, Ha Long Bay was a bit of a flop.        0.0\n",
        "2985  The problem I have is that they charge $11.99 ...        0.0\n",
        "2986  Shrimp- When I unwrapped it (I live only 1/2 a...        0.0\n",
        "2987     It lacked flavor, seemed undercooked, and dry.        0.0\n",
        "2988  It really is impressive that the place hasn't ...        0.0\n",
        "2989  I would avoid this place if you are staying in...        0.0\n",
        "2990  The refried beans that came with my meal were ...        0.0\n",
        "2991         Spend your money and time some place else.        0.0\n",
        "2992  A lady at the table next to us found a live gr...        0.0\n",
        "2993            the presentation of the food was awful.        0.0\n",
        "2994           I can't tell you how disappointed I was.        0.0\n",
        "2995  I think food should have flavor and texture an...        0.0\n",
        "2996                           Appetite instantly gone.        0.0\n",
        "2997  Overall I was not impressed and would not go b...        0.0\n",
        "2998  The whole experience was underwhelming, and I ...        0.0\n",
        "2999  Then, as if I hadn't wasted enough of my life ...        0.0\n",
        "\n",
        "[3000 rows x 2 columns]"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = CountVectorizer()\n",
      "cv_ft = cv.fit_transform(data.review_text)\n",
      "sum_all = np.sum(cv.fit_transform(data.review_text).toarray(), axis = 0)\n",
      "all_tuples = list(zip(sum_all, cv.get_feature_names(), range(sum_all.shape[0])))\n",
      "freq_df = pd.DataFrame.from_records(all_tuples,columns = ['freq','word','word_id'],index = 'word_id')\n",
      "freq_df = freq_df.sort_values(by = 'freq',ascending=False).reset_index()\n",
      "print(freq_df.head)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<bound method NDFrame.head of       word_id  freq          word\n",
        "0        4533  1952           the\n",
        "1         216  1138           and\n",
        "2        2433   789            it\n",
        "3        2427   753            is\n",
        "4        4611   670            to\n",
        "5        4560   643          this\n",
        "6        3098   624            of\n",
        "7        4970   570           was\n",
        "8        2314   400            in\n",
        "9        1829   336           for\n",
        "10       4531   316          that\n",
        "11       3064   306           not\n",
        "12       5071   274          with\n",
        "13       2983   254            my\n",
        "14       4892   245          very\n",
        "15       1993   230          good\n",
        "16       5140   222           you\n",
        "17       3123   221            on\n",
        "18       2023   210         great\n",
        "19        637   201           but\n",
        "20       2113   184          have\n",
        "21       2955   182         movie\n",
        "22        273   180           are\n",
        "23        297   175            as\n",
        "24       4163   171            so\n",
        "25       3323   168         phone\n",
        "26       1742   163          film\n",
        "27        416   150            be\n",
        "28        179   150           all\n",
        "29       3125   146           one\n",
        "...       ...   ...           ...\n",
        "5127     2228     1   hospitality\n",
        "5128     2225     1     horrified\n",
        "5129     2173     1   highlighted\n",
        "5130     2224     1        horrid\n",
        "5131     2176     1         highy\n",
        "5132     2179     1          hilt\n",
        "5133     2181     1       himself\n",
        "5134     2182     1         hinge\n",
        "5135     2184     1          hiro\n",
        "5136     2188     1         hitch\n",
        "5137     2190     1          hits\n",
        "5138     2191     1            ho\n",
        "5139     2192     1        hockey\n",
        "5140     2193     1       hoffman\n",
        "5141     2195     1        holder\n",
        "5142     2198     1          hole\n",
        "5143     2200     1       holiday\n",
        "5144     2201     1     hollander\n",
        "5145     2207     1      homework\n",
        "5146     2208     1      honeslty\n",
        "5147     2211     1         honor\n",
        "5148     2212     1          hook\n",
        "5149     2213     1        hooked\n",
        "5150     2214     1          hoot\n",
        "5151     2216     1         hoped\n",
        "5152     2218     1      hopeless\n",
        "5153     2219     1         hopes\n",
        "5154     2221     1    horrendous\n",
        "5155     2222     1  horrendously\n",
        "5156     5156     1       zombiez\n",
        "\n",
        "[5157 rows x 3 columns]>\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = CountVectorizer(stop_words='english',max_features=250)\n",
      "cv_ft = cv.fit_transform(data.review_text)\n",
      "sum_all = np.sum(cv.fit_transform(data.review_text).toarray(), axis = 0)\n",
      "all_tuples = list(zip(sum_all, cv.get_feature_names(), range(sum_all.shape[0])))\n",
      "freq_df = pd.DataFrame.from_records(all_tuples,columns = ['freq','word','word_id'],index = 'word_id')\n",
      "freq_df = freq_df.sort_values(by = 'freq',ascending=False).reset_index()\n",
      "print(freq_df)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "     word_id  freq           word\n",
        "0         96   230           good\n",
        "1         98   210          great\n",
        "2        138   182          movie\n",
        "3        153   168          phone\n",
        "4         84   163           film\n",
        "5         89   126           food\n",
        "6        118   125           like\n",
        "7        113   119           just\n",
        "8        156   114          place\n",
        "9        214   112           time\n",
        "10       188   108        service\n",
        "11       171   103         really\n",
        "12        12   103            bad\n",
        "13        64    80            don\n",
        "14        16    78           best\n",
        "15       168    66        quality\n",
        "16       225    64             ve\n",
        "17       128    61           love\n",
        "18       240    56           work\n",
        "19       166    56        product\n",
        "20        72    53      excellent\n",
        "21       243    53          works\n",
        "22       142    53           nice\n",
        "23        17    53         better\n",
        "24       193    50          sound\n",
        "25       173    50      recommend\n",
        "26       222    49            use\n",
        "27        56    49            did\n",
        "28       103    48        headset\n",
        "29       212    47          think\n",
        "..       ...   ...            ...\n",
        "220      233    12        watched\n",
        "221       75    12      extremely\n",
        "222       94    12        getting\n",
        "223       82    12        feeling\n",
        "224      227    12         volume\n",
        "225      159    12           plug\n",
        "226       69    12          enjoy\n",
        "227       15    12        believe\n",
        "228      151    12      perfectly\n",
        "229       47    12           crap\n",
        "230      175    12         return\n",
        "231      179    12            saw\n",
        "232       50    12           deal\n",
        "233       25    12         burger\n",
        "234       24    12         buffet\n",
        "235       53    12         design\n",
        "236      197    12        started\n",
        "237       59    12       director\n",
        "238      201    12         sucked\n",
        "239      157    12           play\n",
        "240       61    12  disappointing\n",
        "241      208    12           tell\n",
        "242      109    12     incredible\n",
        "243      102    12         having\n",
        "244      216    12           took\n",
        "245      220    12     understand\n",
        "246      221    12  unfortunately\n",
        "247      125    12        looking\n",
        "248       99    11          hands\n",
        "249      144    11             oh\n",
        "\n",
        "[250 rows x 3 columns]\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "cv = CountVectorizer(stop_words = 'english', max_features=250)\n",
      "cv_ft = cv.fit_transform(data.review_text)\n",
      "lda = LinearDiscriminantAnalysis()\n",
      "lda.fit(cv_ft.toarray(), data.sentiment)\n",
      "coef_df = pd.DataFrame({'word':cv.get_feature_names(),'coef':lda.coef_[0]}).sort_values(by='coef',ascending=False).reset_index(drop=True)\n",
      "print(coef_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "         coef           word\n",
        "0    3.437548      fantastic\n",
        "1    3.422054        perfect\n",
        "2    3.382369      delicious\n",
        "3    3.379798        awesome\n",
        "4    3.282018      beautiful\n",
        "5    3.125905        enjoyed\n",
        "6    3.049498          loved\n",
        "7    3.044426          liked\n",
        "8    2.842215          happy\n",
        "9    2.622447     incredible\n",
        "10   2.535161      excellent\n",
        "11   2.508697    interesting\n",
        "12   2.506219           nice\n",
        "13   2.475099          great\n",
        "14   2.462239           love\n",
        "15   2.455430    comfortable\n",
        "16   2.437315      perfectly\n",
        "17   2.427415      wonderful\n",
        "18   2.376087           easy\n",
        "19   2.276903        amazing\n",
        "20   2.274384          order\n",
        "21   2.228176           best\n",
        "22   2.173981          funny\n",
        "23   1.970077           fine\n",
        "24   1.967920         highly\n",
        "25   1.947991          fresh\n",
        "26   1.905298           cool\n",
        "27   1.892965          enjoy\n",
        "28   1.789015          works\n",
        "29   1.728941           good\n",
        "..        ...            ...\n",
        "220 -1.470173            buy\n",
        "221 -1.525383          waste\n",
        "222 -1.533918   disappointed\n",
        "223 -1.537067           sure\n",
        "224 -1.556459          calls\n",
        "225 -1.568346           plot\n",
        "226 -1.614119        minutes\n",
        "227 -1.632683           didn\n",
        "228 -1.703676         volume\n",
        "229 -1.733668       horrible\n",
        "230 -1.768590           wasn\n",
        "231 -1.796817          worse\n",
        "232 -1.806441       probably\n",
        "233 -1.865991           plug\n",
        "234 -1.907641            old\n",
        "235 -1.962033            bad\n",
        "236 -2.022261        started\n",
        "237 -2.145717          avoid\n",
        "238 -2.150987          piece\n",
        "239 -2.195047          worst\n",
        "240 -2.235140          bland\n",
        "241 -2.235208         sucked\n",
        "242 -2.345304         stupid\n",
        "243 -2.353862           slow\n",
        "244 -2.366651          awful\n",
        "245 -2.479002         return\n",
        "246 -2.522144       terrible\n",
        "247 -2.722753  unfortunately\n",
        "248 -2.989331           poor\n",
        "249 -3.109275  disappointing\n",
        "\n",
        "[250 rows x 2 columns]\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}